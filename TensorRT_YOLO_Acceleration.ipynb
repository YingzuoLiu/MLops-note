{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6af40099",
   "metadata": {},
   "source": [
    "\n",
    "# TensorRT Acceleration for YOLOv5 — End‑to‑End Notebook\n",
    "\n",
    "This notebook shows **how TensorRT achieves inference acceleration** on a real model (**YOLOv5**), with two practical routes:\n",
    "\n",
    "1. **One‑click route (recommended first):** Use YOLOv5's `export.py` to directly generate a TensorRT **engine** (with EfficientNMS integrated).\n",
    "2. **Controllable route:** Export **ONNX** → build **TensorRT FP16 / INT8 engines** via Python APIs (dynamic shapes, timing cache, calibrator) → **async inference** with multiple streams.\n",
    "\n",
    "> **Requirements (run-time environment):**\n",
    "> - NVIDIA GPU (Turing+ recommended, ideally Ampere/ADA)\n",
    "> - CUDA + cuDNN + TensorRT 8.x/9.x installed\n",
    "> - `pycuda`, `numpy`, `torch`\n",
    "> - Internet access to clone YOLOv5 repo (or place it locally)\n",
    "> \n",
    "> **You can run only the parts you need.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e10c34",
   "metadata": {},
   "source": [
    "## 0) Quick Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577ee11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, os, subprocess, platform\n",
    "print(\"Python:\", sys.version)\n",
    "!nvidia-smi || echo \"nvidia-smi not found (ensure NVIDIA drivers are installed)\"\n",
    "try:\n",
    "    import tensorrt as trt\n",
    "    print(\"TensorRT version:\", trt.__version__)\n",
    "except Exception as e:\n",
    "    print(\"TensorRT import failed:\", e)\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "except Exception as e:\n",
    "    print(\"PyTorch import failed:\", e)\n",
    "\n",
    "try:\n",
    "    import pycuda.driver as cuda\n",
    "    import pycuda.autoinit  # noqa: F401\n",
    "    print(\"PyCUDA OK\")\n",
    "except Exception as e:\n",
    "    print(\"PyCUDA import failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfdeeb2",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Get YOLOv5 and Weights\n",
    "\n",
    "- Clone YOLOv5 (official Ultralytics repo).\n",
    "- Download a small model weight (e.g., `yolov5s.pt`).\n",
    "\n",
    "> If you already have the repo and weights, you can skip cloning/downloading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865da090",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, sys\n",
    "WORKDIR = pathlib.Path.cwd() / \"yolov5\"\n",
    "if not WORKDIR.exists():\n",
    "    !git clone --depth=1 https://github.com/ultralytics/yolov5.git\n",
    "else:\n",
    "    print(\"YOLOv5 repo already exists at\", WORKDIR)\n",
    "\n",
    "# Install Python deps for YOLOv5 (optional; comment out if handled elsewhere)\n",
    "%cd yolov5\n",
    "!pip -q install -r requirements.txt || echo \"pip install skipped or failed\"\n",
    "\n",
    "# Get a small weights file\n",
    "WEIGHTS = \"yolov5s.pt\"\n",
    "if not os.path.exists(WEIGHTS):\n",
    "    !python - << 'PY'\n",
    "import torch\n",
    "from urllib.request import urlretrieve\n",
    "url = \"https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt\"\n",
    "try:\n",
    "    urlretrieve(url, \"yolov5s.pt\")\n",
    "    print(\"Downloaded yolov5s.pt\")\n",
    "except Exception as e:\n",
    "    print(\"Download yolov5s.pt failed:\", e)\n",
    "PY\n",
    "else:\n",
    "    print(\"Weights already present:\", WEIGHTS)\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f05767",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Route A — One‑Click Export to TensorRT Engine (with EfficientNMS)\n",
    "\n",
    "YOLOv5's `export.py` supports `--include engine` to produce a TensorRT `.engine` directly.\n",
    "This path often embeds EfficientNMS inside the engine (fast post‑processing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eabf448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd yolov5\n",
    "\n",
    "ENGINE_NAME = \"yolov5s_fp16.engine\"\n",
    "# Fixed 640x640 (stable latency); add --dynamic if you need variable shapes\n",
    "!python export.py --weights yolov5s.pt --include engine --img 640 640 --half --device 0\n",
    "\n",
    "# Move engine to notebook root (optional)\n",
    "import shutil, os\n",
    "if os.path.exists(\"yolov5s.engine\"):\n",
    "    shutil.move(\"yolov5s.engine\", f\"../{ENGINE_NAME}\")\n",
    "    print(\"Engine saved to:\", f\"../{ENGINE_NAME}\")\n",
    "else:\n",
    "    print(\"Engine not found (export may have failed). Check logs above.\")\n",
    "\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044853d",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Route B — Export ONNX from YOLOv5\n",
    "\n",
    "This gives you more control for later building FP16/INT8 engines via the TensorRT Python API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ee14e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%cd yolov5\n",
    "!python export.py --weights yolov5s.pt --include onnx --img 640 640 --opset 13 --simplify --device 0\n",
    "\n",
    "import shutil, os\n",
    "if os.path.exists(\"yolov5s.onnx\"):\n",
    "    shutil.move(\"yolov5s.onnx\", \"../yolov5s.onnx\")\n",
    "    print(\"Exported ONNX to ../yolov5s.onnx\")\n",
    "else:\n",
    "    print(\"ONNX not found. Check export logs.\")\n",
    "%cd ..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62107a3c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Build FP16 TensorRT Engine (Dynamic Shapes + Timing Cache)\n",
    "\n",
    "This cell demonstrates:\n",
    "- **FP16** build flag\n",
    "- **Optimization Profile** (min/opt/max shapes)\n",
    "- **Timing Cache** load/save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095d6080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorrt as trt, os, pathlib\n",
    "\n",
    "ONNX_PATH = \"yolov5s.onnx\"\n",
    "ENGINE_FP16 = \"yolov5s_fp16.plan\"\n",
    "TIMING_CACHE = \"yolo_timing.cache\"\n",
    "\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(logger)\n",
    "flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(flag)\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "assert os.path.exists(ONNX_PATH), \"Missing yolov5s.onnx. Run the ONNX export cell first.\"\n",
    "with open(ONNX_PATH, \"rb\") as f:\n",
    "    parsed = parser.parse(f.read())\n",
    "    if not parsed:\n",
    "        for i in range(parser.num_errors):\n",
    "            print(parser.get_error(i))\n",
    "    assert parsed, \"ONNX parse failed.\"\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 8 << 30)  # 8GB for tactic search\n",
    "\n",
    "if builder.platform_has_fast_fp16:\n",
    "    config.set_flag(trt.BuilderFlag.FP16)\n",
    "\n",
    "# Dynamic shapes (adjust ranges to your needs)\n",
    "profile = builder.create_optimization_profile()\n",
    "profile.set_shape(\"images\",  # YOLOv5 ONNX input name may be 'images' or 'input'; adjust if needed\n",
    "                  min=(1, 3, 320, 320),\n",
    "                  opt=(8, 3, 640, 640),\n",
    "                  max=(16, 3, 1280, 1280))\n",
    "\n",
    "# If your input tensor name differs, fallback to 'input'\n",
    "if not profile.is_valid:\n",
    "    profile = builder.create_optimization_profile()\n",
    "    profile.set_shape(\"input\",\n",
    "                      min=(1, 3, 320, 320),\n",
    "                      opt=(8, 3, 640, 640),\n",
    "                      max=(16, 3, 1280, 1280))\n",
    "\n",
    "config.add_optimization_profile(profile)\n",
    "\n",
    "# Timing cache (reuse kernel tactic results)\n",
    "if os.path.exists(TIMING_CACHE):\n",
    "    with open(TIMING_CACHE, \"rb\") as f:\n",
    "        tc = config.create_timing_cache(f.read())\n",
    "        config.set_timing_cache(tc, ignore_mismatch=True)\n",
    "        print(\"[TimingCache] Loaded\")\n",
    "else:\n",
    "    print(\"[TimingCache] None, will build from scratch\")\n",
    "\n",
    "engine = builder.build_engine(network, config)\n",
    "assert engine, \"Build failed.\"\n",
    "\n",
    "with open(ENGINE_FP16, \"wb\") as f:\n",
    "    f.write(engine.serialize())\n",
    "print(\"[Engine] Saved:\", ENGINE_FP16)\n",
    "\n",
    "tc = config.get_timing_cache()\n",
    "if tc:\n",
    "    blob = tc.serialize()\n",
    "    with open(TIMING_CACHE, \"wb\") as f:\n",
    "        f.write(blob)\n",
    "    print(\"[TimingCache] Saved:\", TIMING_CACHE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b66530",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Optional — Build INT8 Engine with a Minimal Calibrator (PTQ)\n",
    "\n",
    "> For production, **use a representative calibration dataset** and your real preprocessing.  \n",
    "This demo uses random data just to show the **flow**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e401d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorrt as trt, numpy as np, os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # noqa: F401\n",
    "\n",
    "ONNX_PATH = \"yolov5s.onnx\"\n",
    "ENGINE_INT8 = \"yolov5s_int8.plan\"\n",
    "CALIB_CACHE = \"yolo_calib.cache\"\n",
    "\n",
    "class RandomCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, batch_size=8, n_batches=20, h=640, w=640):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.n_batches = n_batches\n",
    "        self.h, self.w = h, w\n",
    "        self.idx = 0\n",
    "        self.cache_file = CALIB_CACHE\n",
    "        self.device_input = cuda.mem_alloc(batch_size * 3 * h * w * np.float32().nbytes)\n",
    "\n",
    "    def get_batch_size(self): return self.batch_size\n",
    "\n",
    "    def get_batch(self, names):\n",
    "        if self.idx >= self.n_batches:\n",
    "            return None\n",
    "        batch = np.random.rand(self.batch_size, 3, self.h, self.w).astype(np.float32)\n",
    "        cuda.memcpy_htod(self.device_input, batch)\n",
    "        self.idx += 1\n",
    "        return [int(self.device_input)]\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            print(\"[Calib] Load cache\")\n",
    "            return open(self.cache_file, \"rb\").read()\n",
    "        return None\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        open(self.cache_file, \"wb\").write(cache)\n",
    "        print(\"[Calib] Save cache\")\n",
    "\n",
    "logger = trt.Logger(trt.Logger.INFO)\n",
    "builder = trt.Builder(logger)\n",
    "flag = 1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "network = builder.create_network(flag)\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "with open(ONNX_PATH, \"rb\") as f:\n",
    "    assert parser.parse(f.read()), \"ONNX parse failed\"\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.set_memory_pool_limit(trt.MemoryPoolType.WORKSPACE, 8 << 30)\n",
    "config.set_flag(trt.BuilderFlag.INT8)\n",
    "if builder.platform_has_fast_fp16:\n",
    "    config.set_flag(trt.BuilderFlag.FP16)  # allow mixed precision\n",
    "\n",
    "# Dynamic profile\n",
    "profile = builder.create_optimization_profile()\n",
    "# try 'images' first, fallback to 'input'\n",
    "ok = True\n",
    "try:\n",
    "    profile.set_shape(\"images\", min=(1,3,320,320), opt=(8,3,640,640), max=(16,3,1280,1280))\n",
    "except Exception as e:\n",
    "    ok = False\n",
    "if not ok:\n",
    "    profile = builder.create_optimization_profile()\n",
    "    profile.set_shape(\"input\", min=(1,3,320,320), opt=(8,3,640,640), max=(16,3,1280,1280))\n",
    "\n",
    "config.add_optimization_profile(profile)\n",
    "config.int8_calibrator = RandomCalibrator()\n",
    "\n",
    "engine = builder.build_engine(network, config)\n",
    "assert engine, \"INT8 build failed\"\n",
    "\n",
    "with open(ENGINE_INT8, \"wb\") as f:\n",
    "    f.write(engine.serialize())\n",
    "print(\"[Engine-INT8] Saved:\", ENGINE_INT8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c628c63",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Async Inference (Two Streams) — FP16 Engine\n",
    "\n",
    "This demonstrates:\n",
    "- Setting dynamic shapes\n",
    "- Using **pinned (page-locked) memory**\n",
    "- **Async H2D → execute → D2H** on **two CUDA streams** (parallelism)\n",
    "- Works with engines that contain EfficientNMS (common when using route A)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057dbe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tensorrt as trt, numpy as np, threading, os\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit  # noqa: F401\n",
    "\n",
    "ENGINE_PATH = \"yolov5s_fp16.plan\" if os.path.exists(\"yolov5s_fp16.plan\") else \"yolov5s_fp16.engine\"\n",
    "if not os.path.exists(ENGINE_PATH):\n",
    "    # If route A engine exists instead (named yolov5s_fp16.engine above)\n",
    "    ENGINE_PATH = \"yolov5s_fp16.engine\"\n",
    "assert os.path.exists(ENGINE_PATH), \"No engine found. Build FP16 or export engine first.\"\n",
    "\n",
    "logger = trt.Logger(trt.Logger.ERROR)\n",
    "with open(ENGINE_PATH, \"rb\") as f, trt.Runtime(logger) as rt:\n",
    "    engine = rt.deserialize_cuda_engine(f.read())\n",
    "\n",
    "# Try typical binding names; adapt as needed depending on your engine\n",
    "input_name_candidates = [\"images\", \"input\"]\n",
    "output_names_hint = []  # if EfficientNMS is present, expect multiple outputs\n",
    "\n",
    "def get_binding_index_by_candidates(engine, names):\n",
    "    for nm in names:\n",
    "        try:\n",
    "            idx = engine.get_binding_index(nm)\n",
    "            if idx != -1:\n",
    "                return idx, nm\n",
    "        except Exception:\n",
    "            pass\n",
    "    # fallback to first input binding\n",
    "    for i in range(engine.num_bindings):\n",
    "        if engine.binding_is_input(i):\n",
    "            return i, engine.get_binding_name(i)\n",
    "    raise RuntimeError(\"No input binding found\")\n",
    "\n",
    "inp_idx, inp_name = get_binding_index_by_candidates(engine, input_name_candidates)\n",
    "print(\"Using input binding:\", inp_name, \"(index:\", inp_idx, \")\")\n",
    "\n",
    "out_indices = [i for i in range(engine.num_bindings) if not engine.binding_is_input(i)]\n",
    "print(\"Output binding count:\", len(out_indices), \"->\", [engine.get_binding_name(i) for i in out_indices])\n",
    "\n",
    "def pagelocked_empty(shape, dtype=np.float32):\n",
    "    return cuda.pagelocked_empty(int(np.prod(shape)), dtype).reshape(shape)\n",
    "\n",
    "def infer_once(engine, shape, tag=\"stream\"):\n",
    "    context = engine.create_execution_context()\n",
    "    context.set_binding_shape(inp_idx, shape)\n",
    "    stream = cuda.Stream()\n",
    "\n",
    "    in_dtype = trt.nptype(engine.get_binding_dtype(inp_idx))\n",
    "    h_in = pagelocked_empty(shape, in_dtype)\n",
    "    h_in[:] = np.random.rand(*shape).astype(in_dtype)\n",
    "\n",
    "    d_in = cuda.mem_alloc(h_in.nbytes)\n",
    "\n",
    "    # Prepare outputs\n",
    "    bindings = [0]*engine.num_bindings\n",
    "    bindings[inp_idx] = int(d_in)\n",
    "\n",
    "    host_out = []\n",
    "    device_out = []\n",
    "    for oi in out_indices:\n",
    "        out_shape = tuple(context.get_binding_shape(oi))\n",
    "        out_dtype = trt.nptype(engine.get_binding_dtype(oi))\n",
    "        h_o = pagelocked_empty(out_shape, out_dtype)\n",
    "        d_o = cuda.mem_alloc(h_o.nbytes)\n",
    "        host_out.append(h_o)\n",
    "        device_out.append(d_o)\n",
    "        bindings[oi] = int(d_o)\n",
    "\n",
    "    cuda.memcpy_htod_async(d_in, h_in, stream)\n",
    "    context.execute_async_v2(bindings, stream.handle)\n",
    "    for h_o, d_o in zip(host_out, device_out):\n",
    "        cuda.memcpy_dtoh_async(h_o, d_o, stream)\n",
    "    stream.synchronize()\n",
    "\n",
    "    print(f\"[{tag}] shapes:\", [arr.shape for arr in host_out])\n",
    "    return host_out\n",
    "\n",
    "# Two parallel requests with different shapes (if dynamic profiles allow it)\n",
    "t1 = threading.Thread(target=infer_once, args=(engine, (1,3,640,640), \"S1\"))\n",
    "t2 = threading.Thread(target=infer_once, args=(engine, (1,3,384,384), \"S2\"))\n",
    "t1.start(); t2.start(); t1.join(); t2.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6276a1",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Notes & Troubleshooting\n",
    "\n",
    "- If ONNX input name differs (e.g., `images` vs `input`), **adjust the code** accordingly.\n",
    "- If your engine doesn't include EfficientNMS, you may see raw predictions and need to run NMS on the host. Prefer using the **engine with NMS integrated** (Route A).\n",
    "- For **INT8**, replace the random calibrator with a **real calibration dataset** matching production distribution.\n",
    "- Use `trtexec` for quick benchmarking and building when you don't need Python APIs:\n",
    "```bash\n",
    "trtexec --onnx=yolov5s.onnx --saveEngine=yolov5s_fp16.plan --fp16 --workspace=16384 \\\n",
    "  --minShapes=images:1x3x320x320 --optShapes=images:8x3x640x640 --maxShapes=images:16x3x1280x1280 \\\n",
    "  --timingCacheFile=yolo_timing.cache --buildOnly --verbose\n",
    "```\n",
    "- Profile with **Nsight Systems/Compute** to locate I/O vs compute bottlenecks.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
