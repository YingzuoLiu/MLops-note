{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cca1019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ========== CONFIG ==========\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"1\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# ----- 参数可调 -----\n",
    "HIDDEN_SIZE = 128       # 隐藏层维度\n",
    "BASE_EPOCHS = 2         # 基线训练 epoch（建议小一些，跑快）\n",
    "BASE_LR     = 0.01\n",
    "BASE_OPTIM  = \"SGD\"     # \"SGD\" or \"Adam\"\n",
    "\n",
    "PRUNE_AMOUNTS = [0.3, 0.5, 0.7, 0.9]  # 剪枝比例\n",
    "\n",
    "FT_EPOCHS  = 2          # 微调 epoch\n",
    "FT_OPTIM   = \"Adam\"     # \"SGD\" or \"Adam\"\n",
    "FT_LR      = 0.001\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "USE_PLOTS  = True\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(1)\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "print(\"Using device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5fa5cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== DATA (MNIST) ==========\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
    "testset  = torchvision.datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "testloader  = DataLoader(testset,  batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e80d6459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== MODEL / UTILS ==========\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils.prune as prune\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, hidden=HIDDEN_SIZE):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, hidden)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "def count_sparsity(model):\n",
    "    total_params, total_zeros = 0, 0\n",
    "    for _, p in model.named_parameters():\n",
    "        numel = p.numel()\n",
    "        zeros = int((p == 0).sum().item())\n",
    "        total_params += numel\n",
    "        total_zeros += zeros\n",
    "    return total_zeros/total_params, total_params, total_zeros\n",
    "\n",
    "def train(model, loader, opt):\n",
    "    model.train()\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        loss = F.cross_entropy(model(X), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "def test(model, loader):\n",
    "    model.eval(); correct=0; total=0\n",
    "    with torch.no_grad():\n",
    "        for X,y in loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(X).argmax(1)\n",
    "            correct += (pred==y).sum().item(); total += y.size(0)\n",
    "    return correct/total\n",
    "\n",
    "def make_optimizer(name, params, lr):\n",
    "    return optim.SGD(params, lr=lr, momentum=0.9) if name==\"SGD\" else optim.Adam(params, lr=lr)\n",
    "\n",
    "def prune_model(model, amount):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            prune.l1_unstructured(m, \"weight\", amount)\n",
    "            prune.remove(m, \"weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400ba9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] acc=0.9260, sparsity=0.000 (0/101770)\n",
      "   prune_amount  sparsity  acc_pruned  acc_ft  zeros_after  total_params  \\\n",
      "0           0.3     0.300      0.9212  0.9447        30490        101770   \n",
      "1           0.5     0.499      0.9148  0.9551        50816        101770   \n",
      "2           0.7     0.699      0.8687  0.9496        71142        101770   \n",
      "3           0.9     0.899      0.7569  0.9508        91469        101770   \n",
      "\n",
      "  ft_optim  ft_lr  \n",
      "0     Adam  0.001  \n",
      "1     Adam  0.001  \n",
      "2     Adam  0.001  \n",
      "3     Adam  0.001  \n"
     ]
    }
   ],
   "source": [
    "# ========== RUN EXPERIMENT (NO PLOTS, SAFE) ==========\n",
    "import gc\n",
    "torch.manual_seed(2024)\n",
    "\n",
    "# 控制每个 epoch 的最大 steps，避免长时间占用内存/线程（可调小/大）\n",
    "MAX_TRAIN_STEPS_PER_EPOCH = 300   # 对 MNIST 128 batch 大约 ~468 steps/epoch\n",
    "\n",
    "def train_one_epoch_capped(model, loader, opt, max_steps=MAX_TRAIN_STEPS_PER_EPOCH):\n",
    "    model.train()\n",
    "    steps = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss = F.cross_entropy(model(X), y)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        steps += 1\n",
    "        if steps >= max_steps:\n",
    "            break\n",
    "    # 显式清理\n",
    "    del X, y, loss\n",
    "    gc.collect()\n",
    "\n",
    "@torch.inference_mode()\n",
    "def test_safe(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X, y in loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(X).argmax(1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.size(0)\n",
    "    # 显式清理\n",
    "    del X, y, pred\n",
    "    gc.collect()\n",
    "    return correct / total\n",
    "\n",
    "# 1) 基线\n",
    "baseline = MLP()\n",
    "opt_base = make_optimizer(BASE_OPTIM, baseline.parameters(), BASE_LR)\n",
    "for _ in range(BASE_EPOCHS):\n",
    "    train_one_epoch_capped(baseline, trainloader, opt_base)\n",
    "\n",
    "acc_base = test_safe(baseline, testloader)\n",
    "s_base, tot_base, zeros_base = count_sparsity(baseline)\n",
    "print(f\"[Baseline] acc={acc_base:.4f}, sparsity={s_base:.3f} ({zeros_base}/{tot_base})\")\n",
    "\n",
    "# 2) 多剪枝比例实验 + 微调\n",
    "records = []\n",
    "for amt in PRUNE_AMOUNTS:\n",
    "    pruned = MLP()\n",
    "    pruned.load_state_dict(copy.deepcopy(baseline.state_dict()))\n",
    "\n",
    "    # L1 剪枝并永久化\n",
    "    prune_model(pruned, amt)\n",
    "\n",
    "    acc_pruned = test_safe(pruned, testloader)\n",
    "    s_after, tot_p, zeros_p = count_sparsity(pruned)\n",
    "\n",
    "    opt_ft = make_optimizer(FT_OPTIM, pruned.parameters(), FT_LR)\n",
    "    for _ in range(FT_EPOCHS):\n",
    "        train_one_epoch_capped(pruned, trainloader, opt_ft)\n",
    "\n",
    "    acc_ft = test_safe(pruned, testloader)\n",
    "\n",
    "    records.append({\n",
    "        \"prune_amount\": amt,\n",
    "        \"sparsity\": round(s_after, 3),\n",
    "        \"acc_pruned\": round(acc_pruned, 4),\n",
    "        \"acc_ft\": round(acc_ft, 4),\n",
    "        \"zeros_after\": zeros_p,\n",
    "        \"total_params\": tot_p,\n",
    "        \"ft_optim\": FT_OPTIM, \"ft_lr\": FT_LR\n",
    "    })\n",
    "\n",
    "    # 清理当前模型占用\n",
    "    del pruned, opt_ft\n",
    "    gc.collect()\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(records)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0b345e",
   "metadata": {},
   "source": [
    "acc_pruned：剪枝后立刻测试的准确率。\n",
    "👉 随着剪枝比例上升，准确率明显下降（90% 剪掉后直接掉到 75.7%）。\n",
    "\n",
    "acc_ft：剪枝后再用训练集微调 2 个 epoch 的结果。\n",
    "👉 几乎完全恢复到 ~95% 左右！即便剪掉 90% 权重，也能恢复到和轻剪枝差不多的水平。\n",
    "\n",
    "剪掉 90% 权重后，仍然能达到 95% 精度 → 说明网络里大部分连接是“多余的”。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
